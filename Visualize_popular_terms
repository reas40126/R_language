Processing tweeter text: Remove redundant information, convert text into corpus, covert to lower case, remove common words or stop words.
rm_twitterr_url() -> Remove URLs
gsub() -> Remove special characters, punctuation & numbers
tm_map() -> using map lower can convert letters to small capital ones
Using removeWords and stopwords("english") function within tm_map to remove stop words
Remove additional spaces by using stripWhitespace in tm_map
Add custom stop words and delete them if needed

# Extract tweet text from the pre-loaded dataset
twt_txt <- twt_telmed$text
head(twt_txt)

# Remove URLs from the tweet text and view the output
twt_txt_url <- rm_twitter_url(twt_txt)
head(twt_txt_url)

# Replace special characters, punctuation, & numbers with spaces
twt_txt_chrs  <- gsub("[^A-Za-z]"," " , twt_txt_url)

# View text after replacing special characters, punctuation, & numbers
head(twt_txt_chrs)

# Convert text in "twt_gsub" dataset to a text corpus and view output
twt_corpus <- twt_gsub %>% 
                VectorSource() %>% 
                Corpus() 
head(twt_corpus$content)

# Convert the corpus to lowercase
twt_corpus_lwr <- tm_map(twt_corpus,tolower) 

# View the corpus after converting to lowercase
head(twt_corpus_lwr$content)

# Remove English stop words from the corpus and view the corpus
twt_corpus_stpwd <- tm_map(twt_corpus_lwr, removeWords, stopwords("english"))
head(twt_corpus_stpwd$content)

# Remove additional spaces from the corpus
twt_corpus_final <- tm_map(twt_corpus_stpwd, stripWhitespace)

# View the text corpus after removing spaces
head(twt_corpus_final$content)

# Create a vector of custom stop words
custom_stopwds <- c("telemedicine", " s", "amp", "can", "new", "medical", 
				"will", "via", "way",  "today", "come", "t", "ways", 
                "say", "ai", "get", "now")

# Remove custom stop words and create a refined corpus
corp_refined <- tm_map(twt_corpus,removeWords, custom_stopwds) 

# Extract term frequencies for the top 20 words
termfreq_clean <- freq_terms(corp_refined, 20)

# Extract term frequencies for the top 10 words
termfreq_10w <- freq_terms(corp_refined, 10)
termfreq_10w

# Identify terms with more than 60 counts from the top 10 list
term60 <- subset(termfreq_10w, FREQ > 60)

# Create a bar plot using terms with more than 60 counts
ggplot(term60, aes(x = reorder(WORD, -FREQ), y = FREQ)) + 
		geom_bar(stat = "identity", fill = "red") + 
		theme(axis.text.x = element_text(angle = 15, hjust = 1))

# Create word cloud with 6 colors and max 50 words
wordcloud(corp_refined, max.words = 50, 
    colors = brewer.pal(6, "Dark2"), 
    scale=c(4,1), random.order = FALSE)
    
# Create a document term matrix (DTM) from the pre-loaded corpus
dtm_climate <- DocumentTermMatrix(corpus_climate)
dtm_climate

# Find the sum of word counts in each document
rowTotals <- apply(dtm_climate, 1, sum)
head(rowTotals)

# Select rows with a row total greater than zero
dtm_climate_new <- dtm_climate[rowTotals > 0, ]
dtm_climate_new

# Create a topic model with 5 topics
topicmodl_5 <- LDA(dtm_climate_new, k = 5)

# Select and view the top 10 terms in the topic model
top_10terms <- terms(topicmodl_5,10)
top_10terms 
